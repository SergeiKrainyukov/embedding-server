# API для работы с документами

## Описание

Сервер теперь поддерживает загрузку MD-документов, их автоматическую векторизацию и интеллектуальный поиск с возвратом кликабельных ссылок на источники.

## Основные возможности

1. **Загрузка MD-файлов** - документы автоматически разбиваются на чанки, векторизуются и сохраняются в БД
2. **Семантический поиск** - поиск релевантных фрагментов по векторному сходству
3. **RAG с источниками** - ответы на вопросы с кликабельными ссылками на источники
4. **Управление документами** - просмотр, удаление документов и их чанков

## API Endpoints

### 1. Загрузка документа

```bash
POST /api/documents/upload
```

**Запрос:**
```json
{
  "fileName": "example.md",
  "content": "# Заголовок\n\nТекст документа..."
}
```

**Ответ:**
```json
{
  "documentId": 1,
  "fileName": "example.md",
  "fileSize": 1234,
  "chunksCreated": 5,
  "createdAt": "2025-11-28T00:23:57.44617"
}
```

### 2. Получить список всех документов

```bash
GET /api/documents
```

**Ответ:**
```json
[
  {
    "id": 1,
    "fileName": "example.md",
    "fileSize": 1234,
    "chunksCount": 5,
    "createdAt": "2025-11-28T00:23:57.44617"
  }
]
```

### 3. Получить информацию о документе

```bash
GET /api/documents/{id}
```

**Ответ:**
```json
{
  "id": 1,
  "fileName": "example.md",
  "fileSize": 1234,
  "chunksCount": 5,
  "createdAt": "2025-11-28T00:23:57.44617"
}
```

### 4. Получить чанки документа

```bash
GET /api/documents/{id}/chunks
```

**Ответ:**
```json
[
  {
    "id": 1,
    "documentId": 1,
    "documentName": "example.md",
    "chunkIndex": 0,
    "text": "Содержимое чанка...",
    "startPosition": 0,
    "endPosition": 500,
    "tokenCount": 125
  }
]
```

### 5. Получить конкретный чанк

```bash
GET /api/documents/{id}/chunks/{chunkIndex}
```

**Ответ:**
```json
{
  "id": 1,
  "documentId": 1,
  "documentName": "example.md",
  "chunkIndex": 0,
  "text": "Содержимое чанка...",
  "startPosition": 0,
  "endPosition": 500,
  "tokenCount": 125
}
```

### 6. Задать вопрос с RAG (главная функция)

```bash
POST /api/documents/ask
```

**Запрос:**
```json
{
  "question": "Какие возможности есть у сервера?",
  "topK": 3
}
```

**Ответ:**
```json
{
  "question": "Какие возможности есть у сервера?",
  "answer": "Сервер имеет следующие основные возможности...",
  "sources": [
    {
      "documentId": 1,
      "documentName": "example.md",
      "chunkIndex": 0,
      "text": "Фрагмент текста из документа...",
      "similarity": 0.85,
      "similarityPercent": "85.0%",
      "link": "http://localhost:8080/api/documents/1/chunks/0"
    }
  ]
}
```

### 7. Удалить документ

```bash
DELETE /api/documents/{id}
```

**Ответ:**
```json
{
  "deleted": true,
  "id": 1
}
```

### 8. Статистика по документам

```bash
GET /api/documents/stats
```

**Ответ:**
```json
{
  "totalDocuments": 5,
  "totalChunks": 23
}
```

## Примеры использования

### Пример 1: Загрузка документа из файла

```bash
# Создайте JSON файл с содержимым
cat > upload.json << 'EOF'
{
  "fileName": "my-document.md",
  "content": "# Мой документ\n\nЭто содержимое моего документа."
}
EOF

# Загрузите документ
curl -X POST http://localhost:8080/api/documents/upload \
  -H "Content-Type: application/json" \
  -d @upload.json
```

### Пример 2: Задать вопрос и получить ответ с источниками

```bash
curl -X POST http://localhost:8080/api/documents/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Расскажи о возможностях сервера",
    "topK": 3
  }'
```

### Пример 3: Получить конкретный чанк по ссылке

```bash
# Используйте ссылку из поля "link" в ответе RAG
curl http://localhost:8080/api/documents/1/chunks/0
```

## Как это работает

1. **Загрузка документа:**
   - Документ разбивается на чанки размером 100-256 токенов с перекрытием 25 токенов
   - Каждый чанк векторизуется через Ollama (модель nomic-embed-text)
   - Чанки с метаданными сохраняются в базу данных

2. **Поиск с RAG:**
   - Вопрос пользователя векторизуется
   - Выполняется поиск топ-K наиболее похожих чанков по косинусному сходству
   - Найденные чанки используются как контекст для генерации ответа через LLM
   - Возвращается ответ вместе со ссылками на источники

3. **Кликабельные ссылки:**
   - Каждый источник содержит поле `link` с прямой ссылкой на чанк
   - По этой ссылке можно получить полный текст чанка с метаданными

## Требования

- Запущенный сервер Ollama с моделью nomic-embed-text
- Запущенный Embedding Server

## Запуск

```bash
# 1. Запустите Ollama
ollama serve

# 2. Загрузите модель (если еще не загружена)
ollama pull nomic-embed-text

# 3. Запустите сервер
./gradlew run
```

Сервер будет доступен по адресу: http://localhost:8080

## Ограничения

- Поддерживаются только MD-файлы
- Максимальный размер чанка: 256 токенов
- Документы хранятся в локальной H2 базе данных
